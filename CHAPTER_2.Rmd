---
title: "CHAPTER_2"
author: "Shreeharsha Shet"
date: "9/10/2018"
output: html_document
---

### Simple linear reg for Advertising data, for each of predictors(TV,Radio & Newspaper)
```{r setup, include=FALSE}
load("Advertising.RData")
load("Income1.Rdata")
load("Income2.Rdata")
load("College.RData")
load("Auto.RData")
summary(Advertising)
```
```{r}
lm.fit.tv = lm(Sales~TV,data=Advertising)
lm.fit.radio = lm(Sales~Radio,data=Advertising)
lm.fit.newspaper = lm(Sales~Newspaper,data=Advertising)
par(mfrow=c(1,3))
plot(Advertising$TV,Advertising$Sales,col=2,xlab = "TV",ylab = "Sales")
abline(lm.fit.tv)
plot(Advertising$Radio,Advertising$Sales,col=3,xlab = "Radio",ylab = "Sales")
abline(lm.fit.radio)
plot(Advertising$Newspaper,Advertising$Sales,col=4,xlab = "Newspaper",ylab = "Sales")
abline(lm.fit.newspaper)
```

_simple linear models used to predict sales using TV, radio, and newspaper, respectively_

Plotting Income as a function of years of education. Followed by 

```{r}
lm.fit.income = lm(Income~poly(Education,3),data = Income1)
summary(lm.fit.income)
```
```{r}
predictedIncome = predict(lm.fit.income,Income1)
par(mfrow=c(1,2))
plot(Income1$Education,Income1$Income,col=2,pch=16,xlab = "Years of Education",ylab = "Income")
plot(Income1$Education,Income1$Income,col=2,pch=16,xlab = "Years of Education",ylab = "Income")
lines(Income1$Education,predictedIncome,col="blue",lwd=2)
segments(Income1$Education,Income1$Income,Income1$Education,predictedIncome,lwd=1)
```
Left : Plotting Years of Education v/s Income, Right: Polynomial fit for the Income data. Black lines represent the errors.
## TODO , rest of fig from ch 2!!


1. For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.

(a) The sample size n is extremely large, and the number of predictors p is small.
_Flexible model will perform better._
(b) The number of predictors p is extremely large, and the number of observations n is small.
_Flexible model will perform worse, would end up overfitting._
(c) The relationship between the predictors and response is highly non-linear.
_Flexible model will perform better._
(d) The variance of the error terms, i.e. σ2 = Var(ε), is extremely high.
_Flexible model will NOT perform better, as it would overfit._

2. Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide n and p.

(a) We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.
_regression & inference , n=500, while predictors are profit, # employees, industry, p=4_

(b) We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each prod- uct we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.
_classification & prediction, n=20 , p=13 (price,budget,comp-price & 10 others_

(c) We are interesting in predicting the % change in the US dollar in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the dollar, the % change in the US market, the % change in the British market, and the % change in the German market.
_regression & prediction, n = # of weeks in 2012, p=3_

3. We now revisit the bias-variance decomposition.
(a) Provide a sketch of typical (squared) bias, variance, training er- ror, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one.
## TODO here in notebook!
(b) Explain why each of the five curves has the shape displayed in part (a).
_bias goes from high -> low as Flixibility increases_
_variance goes from low -> high as Flixibility increases_
_test error initally is high descreases and then after a point increases as Flexibility goes up. Takes U shape_
_training error keeps descreasing with Flixibility_

5. What are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred?
_Very flexible fits the data well. Suitable when all we care about is response. While less flexible is suitable when we need inference(understanding of the model & fit)_

6. Describe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a para- metric approach to regression or classification (as opposed to a non- parametric approach)? What are its disadvantages?
_ parametric assumes a functional form f for the model and reduces problem of finding model to finding a set of paremeters. Can do well with less # of observations as well and is simple to understand . Might not do well in terms of prediction accuracy_

_ non-parametric doesn not assume a functional form. Requires a large # of observations. Does well in terms of prediction accuracy_

7. The table below provides a training data set containing six observa- tions, three predictors, and one qualitative response variable.
(a) Compute the Euclidean distance between each observation and thetestpoint,X1 =X2 =X3 =0.

|Obs|X1|X1|X1|Y|Distance(0,0,0)|
|-|-|-|-|-|-|
|1|0|3|0|Red|sqrt(0+9+0)=>3|
|2|2|0|0|Red|sqrt(4+0+0)=>2|
|3|0|1|3|Red|sqrt(0+1+9)=> ~3.2|
|4|0|1|2|Green|sqrt(0+1+4)=> 2.3|
|5|-1|0|1|Green|sqrt(1+0+1)=> 1.414|
|6|1|1|1|Red|sqrt(1+1+1)=> 1.73|

(b) What is our prediction with K = 1? Why?
_Green, nearest point being #5_
(c) What is our prediction with K = 3? Why?
_Red, 3 nearest points being #5(Green), #6(Red) & #2(Red)_
(d) If the Bayes decision boundary in this problem is highly non- linear, then would we expect the best value for K to be large or small? Why?
_small, small K would fit to non-linear boundary well_

8. This exercise relates to the College data set, which can be found in the file College.csv. It contains a number of variables for 777 different universities and colleges in the US. The variables are

(c)
i. Use the summary() function to produce a numerical summary of the variables in the data set.
```{r}
summary(College)
```

ii. Use the pairs() function to produce a scatterplot matrix of the first ten columns or variables of the data. Recall that you can reference the first ten columns of a matrix A using A[,1:10].

```{r}
pairs(College[,1:10])
```
iii. Use the plot() function to produce side-by-side boxplots of Outstate versus Private.
```{r}
plot(College$Private,College$Outstate)
```
iv. Create a new qualitative variable, called Elite, by binning the Top10perc variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50 %.
```{r}
Elite=rep("No",nrow(College))
Elite[College$Top10perc >50]="Yes"
Elite=as.factor(Elite)
College=data.frame(College ,Elite)
```
Use the summary() function to see how many elite univer- sities there are. Now use the plot() function to produce side-by-side boxplots of Outstate versus Elite.
```{r}
plot(College$Elite,College$Outstate)
summary(College)
```
v. Use the hist() function to produce some histograms with differing numbers of bins for a few of the quantitative vari- ables. You may find the command par(mfrow=c(2,2)) useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways.
```{r}
par(mfrow=c(2,2))
hist(College$Apps)
hist(College$perc.alumni,col=2)
hist(College$S.F.Ratio,breaks=10,col=3)
hist(College$Outstate,breaks=10,col=4)
```

9. This exercise involves the Auto data set studied in the lab. Make sure that the missing values have been removed from the data.
```{r}
Auto= na.omit(Auto)
summary(Auto)
```
(a) Which of the predictors are quantitative, and which are qualitative?

_ quantitative = mpg,displacement,cylinders,horsepower,weight,acceleration _
_ qualitative = year,origin,name _
(b) What is the range of each quantitative predictor? You can an- swer this using the range() function.
```{r}
sapply(Auto[1:6],range)
```
(d) Now remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?
```{r}
summary(Auto[-c(10:85),])
```
(e) Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings.
```{r}
pairs(Auto)
```
9.e
```{r}
par(mfrow=c(2,2))
plot(Auto$mpg~Auto$displacement,main='Auto mpg v/s displacememt')
plot(Auto$mpg~Auto$weight,main='Auto mpg v/s weight')
plot(Auto$mpg~Auto$year,main='Auto mpg v/s year',sub='over years mpg improved')
plot(Auto$mpg~Auto$cylinders,main='Auto mpg v/s cylinders')
```
(f) Suppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer.

_yes, many predictors show co-relation with mpg_

10. This exercise involves the Boston housing data set.
(a) To begin, load in the Boston data set. The Boston data set is
part of the MASS library in R.
```{r}
library(MASS)
summary(Boston)
```

(b) Make some pairwise scatterplots of the predictors (columns) in this data set. Describe your findings.
```{r}
par(mfrow=c(1,2))
plot(Boston$crim,Boston$medv,sub = "less crim => high medv")
plot(Boston$rm,Boston$medv,sub = "more rooms => costs more")
```
(c) Are any of the predictors associated with per capita crime rate? If so, explain the relationship.
```{r}
par(mfrow=c(2,2))
plot(Boston$black,Boston$crim,sub = "no perticular co-relation here")
plot(Boston$age,Boston$crim,sub = "no perticular co-relation here")
plot(Boston$crim,Boston$medv,sub = "no perticular co-relation here")
```
(d) Do any of the suburbs of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor.
```{r}
par(mfrow=c(2,2))
hist(Boston$crim[Boston$crim>1], breaks=25)
hist(Boston$tax, breaks=25)
hist(Boston$ptratio, breaks=25)
```
(e) How many of the suburbs in this data set bound the Charles river?
```{r}
nrow(Boston[Boston$chas==1,])
```
(f) What is the median pupil-teacher ratio among the towns in this data set?
```{r}
median(Boston$ptratio)
```
(g) Which suburb of Boston has lowest median value of owner- occupied homes? What are the values of the other predictors for that suburb, and how do those values compare to the overall ranges for those predictors? Comment on your findings.
```{r}
Boston[Boston$medv==min(Boston$medv),] #min(Boston$medv) => 5
```
(h) In this data set, how many of the suburbs average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the suburbs that average more than eight rooms per dwelling.
```{r}
dim(subset(Boston, rm > 7)) # 64
dim(subset(Boston, rm > 8)) # 13
```
```{r}
summary(subset(Boston, rm > 7))
```

